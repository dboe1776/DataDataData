{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few of My Favorite Things About Pandas (in no particular order)\n",
    "\n",
    "1. Large Datasets\n",
    "     * Pandas has no limit on table size\n",
    "     * Excel is limited to 1,048,576 rows by 16,384 columns (per Microsoft)\n",
    "          * Excel typically becomes sluggish well before approaching this limit\n",
    "          * Excel add-ons can allow for much larger datasets... but, at the end of the day, it's still Excel. \n",
    "\n",
    "2. Format Flexibility \n",
    "     * Pandas can import data from any source (.csv, .xlsx, SQL, etc) and then output data to any source.  \n",
    "     * Pandas can output a .html or $\\LaTeX$ table in one line (a total win)\n",
    "     * Excel can handle most data sources as well, but it prefers its native .xlsx extension which can lead to formatting issues upon importing/exporting.\n",
    "\n",
    "3. Repeatability and Modularity\n",
    "     * Python is a programming language, not a data analysis software package/gui.  \n",
    "         * Custom modules can be built to perform specific tasks repeatedly\n",
    "         * This makes version control a breeze with Github or similar services\n",
    "         * This open's up virtually limitless possibilites for web development, visualizations, etc.\n",
    "     * Excel can be coupled with VBA to accomplish similar tasks (but seriously, why would anyone want to code in VBA when you can basically \"talk\" to your computer with Python?)\n",
    "\n",
    "4. Data Cleaning\n",
    "    * In Pandas, tasks such as elminating bad data, filtering, renaming columns, reindexing, etc, are all one-liners using built-in functions.\n",
    "    * ... I'm scared to even attempt such tasks in Excel \n",
    "\n",
    "5. Joining/Merging\n",
    "     * Taking two (or more) separates datasets and combining them into one is a simple procedure in Pandas\n",
    "     * Excel does not really perform straight up SQL style joins.  There are definitely workarounds, but they are rather slow and certainly less straightforward\n",
    "\n",
    "6. Aggregation\n",
    "    * Pandas' \"split-apply-combine\" strategy allows for any function to be easily applied to groups of data resulting in a rapid method for summarizing data by group.\n",
    "\n",
    "7. Data Visualization \n",
    "    * Python's Matplotlib library can be used to create publication quality graphics.\n",
    "        * __All__ elements of a plot can be modified\n",
    "        * Graphics can be easily exported to vector image format.\n",
    "    * Other Python plotting modules (Bokeh, Plotly, Plotly Express, etc) allow for interactive graphics.\n",
    "    * Plots generated in Excel .... look like they were generated in Excel (which is not good)\n",
    "        * Preset chart formats can look okay, but customization is very difficult to non-existent.\n",
    "\n",
    "8. Documentation\n",
    "    * Python has an extraordinary (and mostly friendly) open source community that has already answered 99.9% of any questions you could possibly have (which means, most code as already been written for you).\n",
    "\n",
    "9. Syntax \n",
    "    * Python is high level language with a natural, readable syntax\n",
    "        * This becomes very useful when collaborating on projects - well written python code is easy to follow.\n",
    "    * Excel - What sadistic developer thought dollar signs were a good idea??\n",
    "\n",
    "10. Cost\n",
    "    * Python = Free\n",
    "    * Excel = Not Free\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pandas DataFrame \n",
    "\n",
    "Pandas is quite likely the most important python module available to engineers; it is a data science tool that utilizes the python language to provide a powerful framework for processing, analyzing and presenting data.  At the very heart of pandas is an object called a DataFrame - a pandas.DataFrame() to be exact.  The dataframe serves as the storehouse for all your data; it is essentially a massive table in which you have columns that represent data fields and rows that represent a particular observance of these data fields, that is each row is indexed by some referance field that indicates when or where each row of data was measured.  These data can come from any source; they can imported from a .csv file, queried from a SQL Server, or generated right in the python session.\n",
    "\n",
    "To introduce the dataframe object, let's consider the case of plotting some function such as $f(x)=\\frac{cos(x)}{1+x}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data within a script\n",
    "\n",
    "Begin (as most python scripts do) by importing the modules you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed as follows:\n",
    "1. Create the input column, $x$, using the *linspace* function from the numpy package.  This function (identical to linspace MatLab users will be familiar with) just generates an array of equally spaced values between two specified numbers; the syntax is *np.linspace(Start,Stop,NumberOfPoints)*\n",
    "2. Create a *dictionary* which contains three keys, *Function1*, *Function2*, and *Input*.  Each key is basically just a label for each corresponding variable contained within the dictionary, so rather than just creating an *array* of three different variables, we create a *dictionary* of three different variables where each variable is accessed using its key rather than a positional index.  In this case, each variable is an array; the *Input* variable is just the $x$ values we already made.  Let's make *Function1* be a decaying $cosine$ function, and let's have *Function2* be a decaying $sine$ function\n",
    "3. Convert the variable $Data$ (which is a dictionary object) into a pandas *DataFrame* object called $DF$.  Dictionaries are native python objects that are useful for storing data, but very useful for operating upon data - that is what the DataFrame object is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input array\n",
    "x=np.linspace(0,6*np.pi,1000)\n",
    "\n",
    "# Create Dictionary that stores input as well as output of both functions\n",
    "Data={'Function1':np.cos(x)*1/(1+x),'Function2':np.sin(x)*1/(1+x),'Input':x}\n",
    "\n",
    "# Convert the Dictionary into a dataframe\n",
    "DF=pd.DataFrame(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at our DataFrame by typing in the variable name, $DF$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be in order.  Note that we have three labeled columns and 1000 rows (it says 999 as the last index, but don't forget that it starts at 0).\n",
    "\n",
    "There are two ways we can access a specfic column of a *DataFrame*; we can either use brackets to specificy the column name as a string or we can use  a period to specify the column name as an attribute of the *DataFrame* object. For instance, either line below will return the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['Function1']\n",
    "DF.Function1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize the *Matplotlib* package to create some nice plots of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''Create figure and axes objects'''\n",
    "fig=plt.figure(dpi=125)\n",
    "ax=plt.axes()\n",
    "\n",
    "'''Generate the plots and add a grid to the axis'''\n",
    "plt.plot(DF.Input,DF.Function1,color='saddlebrown',linewidth=3,label='Function1')\n",
    "plt.plot(DF.Input,DF.Function2,color='seagreen',linewidth=3,label='Function2')\n",
    "plt.grid()\n",
    "\n",
    "'''Add a legend and set the axis limits and labels'''\n",
    "ax.legend(loc='upper right',fontsize=16,facecolor='white',framealpha=0.25)\n",
    "ax.set_xlim(right = DF.Input.iloc[-1] , left = DF.Input.iloc[0])\n",
    "ax.set_ylim(top=1)\n",
    "ax.set_ylabel('Response')\n",
    "ax.set_xlabel('Time')\n",
    "\n",
    "'''Create a title for the figure'''\n",
    "fig.suptitle('Example of Plotting in Matplotlib', fontsize=16, weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets generate a cool plot using the *fill_between()* function from *matplotlib*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create a new set of figure and axes objects'''\n",
    "fig2=plt.figure(dpi=125)\n",
    "ax2=plt.axes()\n",
    "\n",
    "'''Generate the plots and add a grid to the axis'''\n",
    "plt.fill_between(DF.Input,DF.Function1,color='steelblue',alpha=0.3,linewidth=0,label='Function1')\n",
    "plt.fill_between(DF.Input,DF.Function2,color='slategrey',alpha=0.3,linewidth=0,label='Function2')\n",
    "plt.grid()\n",
    "\n",
    "'''Add a legend and set the axis limits and labels'''\n",
    "ax2.legend(loc='upper right',fontsize=16,facecolor='white',framealpha=0.25)\n",
    "ax2.set_xlim(right = DF.Input.iloc[-1] , left = DF.Input.iloc[0])\n",
    "ax2.set_ylim(top=1)\n",
    "ax2.set_ylabel('Response')\n",
    "ax2.set_xlabel('Time')\n",
    "\n",
    "'''Create a title for the figure'''\n",
    "fig2.suptitle('Example of Plotting in Matplotlib with fill_between()', fontsize=16,weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Importing Data From a .csv File\n",
    "\n",
    "Next, lets play with some actual data from a project I did for my ME 553 Feeback Control Systems class.  For this project, I was using an Arduino to control the temperature of a heating element; the goal was to use PID control to reach and then maintain a certain target (50$^{\\circ}$) temperature. To assist in the project analysis, I wrote a python script that logged serial output from an Arduino into a .csv file.  The Arduino measured and output the following parameters to the log file:\n",
    "\n",
    "1. Time at which the sample was taken\n",
    "2. Ambient Temperature\n",
    "3. Heating Element Temperature\n",
    "4. Duty Cycle of the Heating Element\n",
    "\n",
    "We can open these data from the .csv file using the Pandas function *read_csv()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Store the name of the csv file - note that it has to be in our current directory if we just supply\n",
    "the name, otherwise we have to supply the entire path to the file; however, there are some super nice\n",
    "python modules that make this task pretty easy.'''\n",
    "\n",
    "FileName='TempData.csv'\n",
    "\n",
    "'''Create the DataFrame'''\n",
    "DF2=pd.read_csv(FileName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to verify that the data loaded in properly, a handy function to use is the *head()* function  wich is a method of a *DataFrame* object. The *head* function just returns the first five rows of the DataFrame we access it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our *DataFrame* object named DF2 and we call the *head()* function through this dataframe using the '.' followed by the function name.  This is how functions specific to *DataFrame* objects can be called - *head()* is just one of many, many functions that can be called through a *DataFrame* object. For instance, we could type *DF2.tail()* to get the last five rows of the dataframe or we could type *DF2.max()* to get the maximum value of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, another really useful function is *describe()*. This one gives us a summary of the dataset by returning several statistics about each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *count* value (which gives the number of rows in the column) for Ambient is 1313 while the *counts* for Heater and DutyCycle are 1312 and 1019 respectively.  This is because the Heater and Duty Cycle columns contain some *NAN* values which are not counted... fortunately, pandas has a really slick function for solving this problem. Let's type in *DF2.dropna(how='any').head()* and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.dropna(how='any').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! the *NAN* values are gone now.  The *dropna()* function allows us to drop rows that contain *NAN* values from the dataframe, and the cool thing is we can specify how we want to deal with *NAN*. For our case, we wanted to eliminate rows that contain any *NAN*, so we passed the argument *how ='any'* to *dropna()*. If we only wanted to drop rows where all the values in that row are *NAN*, then we could have just specified *how='all'*.\n",
    "\n",
    "Another item to note is that we called two functions in sequence on *DF2*; we first called *dropna* and then called *head*. This can be useful, but note that these these functions are called sequentially meaning that the first function is executed on the calling dataframe and then the second function is executed on the dataframe that is returned by the first function.\n",
    "\n",
    "We just ran the *dropna()* function on the *DF2* dataframe, but what happens to *DF2*? is it changed at all?  The variable *DF2* was created and stored in memory when we first created it using the *read_csv()* function.  When we called the *head*, *tail*, *describe*, and *dropna* functions on *DF2*, those functions had no impact on *DF2*; they just returned a new *DataFrame* object that is not stored in memory.  If we want to change *DF2*, then we can just set *DF2* equal to this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2=DF2.dropna(how='any')\n",
    "DF2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now call *describe* on *DF2*, we see that we have made the desired changes to *DF2*; however, it should be noted that the Time column is missing, that is, *describe* returns no summaries on the Time column.  This is because Time is a non-numeric type, specifically, Time contains data of type *string*, and obiously, statistical summaries do not pertain to *strings*.  This leads to a very important conept in pandas and in programming in general - Data Types.  Every column in a *DataFrame* has a specific *type* associated with it.  When we import from a .csv file, Pandas automatically converts any numeric data straight to an appropriate *type* (typically, *float64*); however, non-numeric data are left as *string*.  We can check the data types of our dataframe using the *dtypes* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Ambient, Heater, and Duty Cycle are all *float64* while Time is just *object*. For our case, we want Time to be of type *datetime* as the *datetime* type allows us to perform many useful operations on the timstamp; we can accomplish this by using the *to_datetime()* function from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2['Time'] = pd.to_datetime(DF2.Time)\n",
    "DF2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data types again, we now see that the Time column is *datetime64*, which is what we want - a timestamp for each row.  However, if we look once more at the dataframe, we note that there seems to be repeat values for some timestamps. For instance, note that there are three entrires with at timestamp of _2019-04-25 18\\:34\\:22_.  This behavior arises from a (since fixed) bug in my arduino logging program; I had reduced the sampling time and failed to increase the precision of the timestamp to include milliseconds.  Since I would prefer to not have three separate temperature readings corresponding to one second, we can have pandas resample the entire dataframe with a one second frequency - taking the average value of any repeated time stamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Resample the dataframe - note we have to reset the index in order to retain Time as a column ... it is mildy annoying'''\n",
    "DF2=DF2.resample('1s',on='Time').mean().reset_index()\n",
    "\n",
    "\n",
    "DF2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, note that the repeated timestamps are gone and we now have only one entry per second.  Since we would eventually like to generate a plot that shows the time response of the heating element, we should create a new columns that gives the time elapsed rather than an absolute timestamp.  That is, we want a column that begins at $t=0$ and gives the number of seconds since this initial timestamp.  There are a few ways to accomplish this, but here is how I typically create this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2['TimeElapsed'] = (DF2.Time-DF2.Time.iloc[0]).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line is just subtracting off the first Timestamp in the Time column from every element in the Time column - that is what *(DF2.Time-DF2.Time.iloc[0])* does.  Next, we use the *dt* operator to access the *total_seconds()* function which converts each time difference into seconds - giving us a running count of time elapsed.  \n",
    "\n",
    "Also, it should be noted that in order to extract the first element in the Time column, we use *DF2.Time.iloc[0]*.  The *iloc[n]* accessor is very useful as it just grabs the *nth* elementh of specified column.  So, for instance, *DF2.Time.iloc[10]* would give us the tenth element in the Time column and *DF2.Time.iloc[-1]* would actually give us the last element in the Time column.\n",
    "\n",
    "Let's check what our Dataframe looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with time dependent data, it would make sense to index by our TimeElapsed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.set_index('TimeElapsed',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's generate a time series plot for these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create figure and axes objects'''\n",
    "fig3=plt.figure(dpi=125)\n",
    "ax3=plt.axes()\n",
    "\n",
    "'''Generate the plots and add a grid to the axis'''\n",
    "plt.plot(DF2.index/60,DF2.Ambient,color='darkorange',linewidth=3,label='Ambient')\n",
    "plt.plot(DF2.index/60,DF2.Heater,color='dodgerblue',linewidth=3,label='Heater')\n",
    "plt.grid()\n",
    "\n",
    "'''Add a legend and set the axis limits and labels'''\n",
    "ax3.legend(loc='center right',fontsize=16,facecolor='white',framealpha=0.25)\n",
    "ax3.set_xlim(right = DF2.index[-1]/60 , left = DF2.index[0]/60)\n",
    "ax3.set_ylim(bottom=10,top=DF2.Heater.max()+2)\n",
    "ax3.set_ylabel('Temperature [$^{\\circ}C$]')\n",
    "ax3.set_xlabel('Time [Minutes]')\n",
    "\n",
    "'''Create a title for the figure'''\n",
    "fig3.suptitle('Plot of Demo Temperature Data', fontsize=16, weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have looked at the basic mechanics of the Pandas Dataframe.\n",
    "* What a DataFrame is\n",
    "* How to create a DataFame\n",
    "    * From an array of data generated in python\n",
    "    * From data stored in a .csv file\n",
    "* Basic data processing operations\n",
    "    * How to handle *NAN*\n",
    "    * How to identify column data types\n",
    "    * How to change column data types (string to datetime)\n",
    "    * How to redindex using a time series column\n",
    "    * How to change the index\n",
    "    * How to create a cumulative time series column \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Pandas also supports an excpetionally useful function for generating tables in $\\LaTeX$ and HTML.  This code will generate just very basic tables - the $\\LaTeX$ one isn't too bad looking, but the HTML one looks like it belongs on a Windows 95 machine. We could alter the appearance of this table by writing a .css file that supplies the table styling we want, and then tell pandas to use this style class for rendering the table. This process can be used to create sleek, interactive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TexSampleTable.tex', 'w') as file:\n",
    "     file.write(DF2.describe().to_latex(float_format=\"{:0.2f}\".format))\n",
    "\n",
    "with open('HTMLSampleTable.html', 'w') as file:\n",
    "     file.write(DF2.describe().to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
